# LAD: Length-Adaptive Distillation
This repo provides the implementation of our work *Length-Adaptive Distillation: Customizing Small Language Model for Dynamic Token Pruning* published in Findings of EMNLP 2023.

Our implementation is mainly based on [transformers](https://github.com/huggingface/transformers). We using the same data augmentation code provided by [TinyBERT](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT). We following [LAT](https://github.com/clovaai/length-adaptive-transformer) to calculate the speedup ratio.
